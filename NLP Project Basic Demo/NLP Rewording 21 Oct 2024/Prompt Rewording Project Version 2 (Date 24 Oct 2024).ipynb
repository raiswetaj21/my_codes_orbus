{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe1e929",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd74dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\adity\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac170ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0333d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f5d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_text_with_context(text, format_choice='paragraph'):\n",
    "    # Define the prompt to guide the paraphrasing\n",
    "    context_prompt = \"Rephrase this tender document's scope to reflect the bidder's responsibilities for an SRS document: \"\n",
    "\n",
    "    # If there are bullet points, paraphrase them point by point\n",
    "    if \"1.\" in text:\n",
    "        bullet_points = text.split(\"\\n\")\n",
    "        paraphrased_points = []\n",
    "        \n",
    "        for point in bullet_points:\n",
    "            if point.strip():  # Skip empty lines\n",
    "                input_text = f\"paraphrase: {context_prompt} {point.strip()} </s>\"\n",
    "                input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "                outputs = model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
    "                paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                paraphrased_points.append(paraphrased_text)\n",
    "        \n",
    "        if format_choice == 'bullet_points':\n",
    "            return paraphrased_points\n",
    "        else:\n",
    "            return \" \".join(paraphrased_points)\n",
    "    else:\n",
    "        # Paraphrase the entire text with context\n",
    "        input_text = f\"paraphrase: {context_prompt} {text.strip()} </s>\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
    "        paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return paraphrased_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68dd3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input scope paragraph (with bullet points)\n",
    "scope_paragraph = \"\"\"\n",
    "The minimum specified scope of work to be undertaken by the bidder for setting up and operating ISWM Project is mentioned below. The Selected Bidder shall develop, deploy, integrate and support the required deliverables as per the scope and schedule of the contract along with the installation of hardware as detailed out in the further sections.\n",
    "As a part of the technical proposal bidder is expected to submit the proposed systems complete technology stack & architecture.\n",
    "After signing of the Agreement, the Systems Integrator needs to deploy the team proposed within 15 days of receiving the work order for the Project and ensure that a Project Inception Report is submitted to PMC which should cover following aspects:\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project (which should be in line with what has been proposed during bidding stage but may have value additions / learning in the interest of the project).\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3235d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the format you want ('bullet_points' or 'paragraph'): bullet_points\n"
     ]
    }
   ],
   "source": [
    "# User input for format preference\n",
    "format_choice = input(\"Enter the format you want ('bullet_points' or 'paragraph'): \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a241f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase and display output with custom context\n",
    "paraphrased_output = paraphrase_text_with_context(scope_paragraph, format_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0962e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paraphrased Bullet Points:\n",
      "1. True\n",
      "2. True\n",
      "3. True\n",
      "4. True\n",
      "5. True\n",
      "6. True\n",
      "7. True\n",
      "8. True\n"
     ]
    }
   ],
   "source": [
    "if format_choice == 'bullet_points':\n",
    "    print(\"\\nParaphrased Bullet Points:\")\n",
    "    for i, bullet in enumerate(paraphrased_output, 1):\n",
    "        print(f\"{i}. {bullet}\")\n",
    "else:\n",
    "    print(\"\\nParaphrased Paragraph:\")\n",
    "    print(paraphrased_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9dfe3",
   "metadata": {},
   "source": [
    "# Version 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f765be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We’re not just paraphrasing; we’re rewriting the scope into the language of a bidder’s responsibility, \n",
    "suitable for the SRS document that is being extracted from the Tender document.'''\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eeeac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e8ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for transforming tender text to SRS language\n",
    "def transform_to_srs(text, format_choice='paragraph'):\n",
    "    # Define a prompt to guide the transformation to bidder's responsibilities in SRS context\n",
    "    context_prompt = (\"Convert this tender document scope into the deliverables and obligations \"\n",
    "                      \"the bidder will take on in an SRS document: \")\n",
    "    \n",
    "    # Check if the text contains bullet points (starts with numbered lines)\n",
    "    if \"1.\" in text:\n",
    "        # Split by newlines to isolate bullet points\n",
    "        bullet_points = text.split(\"\\n\")\n",
    "        transformed_points = []\n",
    "        \n",
    "        for point in bullet_points:\n",
    "            if point.strip():  # Skip empty lines\n",
    "                input_text = f\"paraphrase: {context_prompt} {point.strip()} </s>\"\n",
    "                input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "                outputs = model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
    "                transformed_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                transformed_points.append(transformed_text)\n",
    "        \n",
    "        # Return based on user's choice\n",
    "        if format_choice == 'bullet_points':\n",
    "            return transformed_points\n",
    "        else:\n",
    "            return \" \".join(transformed_points)\n",
    "    else:\n",
    "        # If no bullet points, transform the entire text with context\n",
    "        input_text = f\"paraphrase: {context_prompt} {text.strip()} </s>\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
    "        transformed_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return transformed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9961d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input scope from tender\n",
    "scope_paragraph = \"\"\"\n",
    "The minimum specified scope of work to be undertaken by the bidder for setting up and operating ISWM Project is mentioned below. The Selected Bidder shall develop, deploy, integrate and support the required deliverables as per the scope and schedule of the contract along with the installation of hardware as detailed out in the further sections.\n",
    "As a part of the technical proposal bidder is expected to submit the proposed systems complete technology stack & architecture.\n",
    "After signing of the Agreement, the Systems Integrator needs to deploy the team proposed within 15 days of receiving the work order for the Project and ensure that a Project Inception Report is submitted to PMC which should cover following aspects:\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project (which should be in line with what has been proposed during bidding stage but may have value additions / learning in the interest of the project).\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996569bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter format ('bullet_points' or 'paragraph'): paragraph\n"
     ]
    }
   ],
   "source": [
    "# User input for format choice\n",
    "format_choice = input(\"Enter format ('bullet_points' or 'paragraph'): \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f395ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform tender to SRS content\n",
    "srs_output = transform_to_srs(scope_paragraph, format_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17c73da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed SRS Paragraph:\n",
      "True False True True False True True False\n"
     ]
    }
   ],
   "source": [
    "if format_choice == 'bullet_points':\n",
    "    print(\"\\nTransformed SRS Bullet Points:\")\n",
    "    for i, bullet in enumerate(srs_output, 1):\n",
    "        print(f\"{i}. {bullet}\")\n",
    "else:\n",
    "    print(\"\\nTransformed SRS Paragraph:\")\n",
    "    print(srs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528d978",
   "metadata": {},
   "source": [
    "# Version 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5a62f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS-Style Bidder Responses:\n",
      "1. We will fulfill the requirements related to '1. Names of the Project Team members, their roles & responsibilities' as specified in the tender.\n",
      "2. We will fulfill the requirements related to '2. Approach & methodology to be adopted to implement the Project' as specified in the tender.\n",
      "3. We will fulfill the requirements related to '3. Responsibility matrix for all stakeholders' as specified in the tender.\n",
      "4. We will fulfill the requirements related to '4. Risks the bidder anticipates and the plans they have towards their mitigation.' as specified in the tender.\n",
      "5. We will fulfill the requirements related to '5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.' as specified in the tender.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_bidder_response(tender_text):\n",
    "    # List to store the transformed output\n",
    "    bidder_responses = []\n",
    "    \n",
    "    # Mapping common phrases to bidder's language\n",
    "    response_mapping = {\n",
    "        \"Names of the Project Team members, their roles & responsibilities\": \"We will submit the list of Project Team members, detailing each member's role and their specific responsibilities within the project.\",\n",
    "        \"Approach & methodology to be adopted to implement the Project\": \"We will adopt the proposed approach and methodology for implementing the project, aligned with the bidding stage and including any additional value we can add for the project’s success.\",\n",
    "        \"Responsibility matrix for all stakeholders\": \"We will create a responsibility matrix outlining the roles and responsibilities of all stakeholders involved in the project.\",\n",
    "        \"Risks the bidder anticipates and the plans they have towards their mitigation\": \"We will identify potential risks and propose a mitigation plan to address those risks during project implementation.\",\n",
    "        \"Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines\": \"We will develop a detailed project plan, identifying dependencies between activities and sub-activities, along with corresponding timelines for completion.\"\n",
    "    }\n",
    "    \n",
    "    # Split the input into lines (handling bullet points)\n",
    "    tender_lines = tender_text.split(\"\\n\")\n",
    "    \n",
    "    # Process each line or bullet point\n",
    "    for line in tender_lines:\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        if stripped_line in response_mapping:\n",
    "            # If the line matches the predefined mapping, transform it\n",
    "            bidder_responses.append(response_mapping[stripped_line])\n",
    "        elif stripped_line:  # Ensure we don't process empty lines\n",
    "            # Generic response for unmatched lines (further customization can be added)\n",
    "            bidder_responses.append(f\"We will fulfill the requirements related to '{stripped_line}' as specified in the tender.\")\n",
    "    \n",
    "    return bidder_responses\n",
    "\n",
    "# Example tender content (bullet points or lines from the document)\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Convert tender text to SRS-style bidder response\n",
    "srs_responses = convert_to_bidder_response(tender_text)\n",
    "\n",
    "# Print the output in bullet point format\n",
    "print(\"SRS-Style Bidder Responses:\")\n",
    "for i, response in enumerate(srs_responses, 1):\n",
    "    print(f\"{i}. {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac8c32",
   "metadata": {},
   "source": [
    "# Version 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0caa8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS-Style Bidder Responses:\n",
      "1. We will submit the list of Project Team members, detailing each member's role and their specific responsibilities within the project.\n",
      "2. We will adopt the proposed approach and methodology for implementing the project, aligned with the bidding stage and including any additional value we can add for the project’s success.\n",
      "3. We will create a responsibility matrix outlining the roles and responsibilities of all stakeholders involved in the project.\n",
      "4. We will identify potential risks and propose a mitigation plan to address those risks during project implementation.\n",
      "5. We will develop a detailed project plan, identifying dependencies between activities and sub-activities, along with corresponding timelines for completion.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced substring matching for SRS responses\n",
    "def convert_to_bidder_response_v2(tender_text):\n",
    "    # List to store the transformed output\n",
    "    bidder_responses = []\n",
    "\n",
    "    # Define key phrases that will map to more specific bidder responses\n",
    "    response_mapping = {\n",
    "        \"Names of the Project Team members\": \"We will submit the list of Project Team members, detailing each member's role and their specific responsibilities within the project.\",\n",
    "        \"Approach & methodology to be adopted\": \"We will adopt the proposed approach and methodology for implementing the project, aligned with the bidding stage and including any additional value we can add for the project’s success.\",\n",
    "        \"Responsibility matrix for all stakeholders\": \"We will create a responsibility matrix outlining the roles and responsibilities of all stakeholders involved in the project.\",\n",
    "        \"Risks the bidder anticipates\": \"We will identify potential risks and propose a mitigation plan to address those risks during project implementation.\",\n",
    "        \"Detailed Project Plan\": \"We will develop a detailed project plan, identifying dependencies between activities and sub-activities, along with corresponding timelines for completion.\"\n",
    "    }\n",
    "    \n",
    "    # Split the input into lines (handling bullet points)\n",
    "    tender_lines = tender_text.split(\"\\n\")\n",
    "    \n",
    "    # Process each line or bullet point\n",
    "    for line in tender_lines:\n",
    "        matched = False\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        for key_phrase, response in response_mapping.items():\n",
    "            if key_phrase in stripped_line:  # Check for substring match\n",
    "                bidder_responses.append(response)\n",
    "                matched = True\n",
    "                break  # Once matched, no need to check other phrases\n",
    "\n",
    "        if not matched and stripped_line:  # Generic response if no match found\n",
    "            bidder_responses.append(f\"We will fulfill the requirements related to '{stripped_line}' as specified in the tender.\")\n",
    "    \n",
    "    return bidder_responses\n",
    "\n",
    "# Example tender content (bullet points or lines from the document)\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Convert tender text to SRS-style bidder response\n",
    "srs_responses = convert_to_bidder_response_v2(tender_text)\n",
    "\n",
    "# Print the output in bullet point format\n",
    "print(\"SRS-Style Bidder Responses:\")\n",
    "for i, response in enumerate(srs_responses, 1):\n",
    "    print(f\"{i}. {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25797b5e",
   "metadata": {},
   "source": [
    "# Version 2.4\n",
    "\n",
    "AI-Driven SRS Document Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e8b91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e1d76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # or use 't5-large' for better performance\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ae2ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate bidder's SRS response from tender text\n",
    "def generate_bidder_srs_response(tender_text):\n",
    "    # Define the task prompt to guide the model for correct paraphrasing\n",
    "    prompt = (\"Convert this tender document text into a bidder's response perspective \"\n",
    "              \"suitable for inclusion in an SRS document: \")\n",
    "    \n",
    "    # Add the tender text to the prompt\n",
    "    input_text = prompt + tender_text\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the bidder's response using T5\n",
    "    outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the output back into text\n",
    "    srs_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return srs_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de695d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tender content (bullet points or lines from the document)\n",
    "tender_text = \"\"\"\n",
    "The minimum specified scope of work to be undertaken by the bidder for setting up and operating ISWM Project is mentioned below.\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7617640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bidder's response for SRS document\n",
    "srs_output = generate_bidder_srs_response(tender_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e9d2e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS-Style Bidder Response:\n",
      "\n",
      "Convert this tender document text into a bidder's response perspective suitable for inclusion in an SRS document: Convert this tender document text into a bidder's response perspective suitable for inclusion in an SRS document:\n"
     ]
    }
   ],
   "source": [
    "# Print the output\n",
    "print(\"SRS-Style Bidder Response:\\n\")\n",
    "print(srs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a184b",
   "metadata": {},
   "source": [
    "# Version 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab61d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS-Style Bidder Response:\n",
      "\n",
      "a bidder who has won the tender, paraphrase the following tender document into a formal SRS response . the minimum specified scope of work to be undertaken by the bidder for setting up and operating ISWM Project is mentioned below.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # You can switch to 't5-large' if needed\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate bidder's SRS response from tender text\n",
    "def generate_bidder_srs_response(tender_text):\n",
    "    # Updated task prompt for clearer paraphrasing instructions\n",
    "    prompt = (\"As a bidder who has won the tender, paraphrase the following tender document \"\n",
    "              \"into a formal SRS response: \")\n",
    "    \n",
    "    # Combine the prompt with the tender text\n",
    "    input_text = prompt + tender_text\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the bidder's response using T5\n",
    "    outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the output back into text\n",
    "    srs_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return srs_response\n",
    "\n",
    "# Example tender content (bullet points or lines from the document)\n",
    "tender_text = \"\"\"\n",
    "The minimum specified scope of work to be undertaken by the bidder for setting up and operating ISWM Project is mentioned below.\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate bidder's response for SRS document\n",
    "srs_output = generate_bidder_srs_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"SRS-Style Bidder Response:\\n\")\n",
    "print(srs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d94636",
   "metadata": {},
   "source": [
    "# Version 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20509898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS-Style Bidder Response:\n",
      "\n",
      "a bidder's response for the following tender requirements: 1. Names of the Project Team members, their roles & responsibilities 2. Approach & methodology to be adopted to implement the Project 4. Risks the bidder anticipates and the plans they have towards their mitigation. 6. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # You can switch to 't5-large' for larger inputs\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate bidder's SRS response from tender text\n",
    "def generate_bidder_srs_response(tender_text):\n",
    "    # Simplified task prompt\n",
    "    prompt = \"Generate a bidder's response for the following tender requirements: \"\n",
    "    \n",
    "    # Combine the prompt with the tender text\n",
    "    input_text = prompt + tender_text\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the bidder's response using T5\n",
    "    outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the output back into text\n",
    "    srs_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return srs_response\n",
    "\n",
    "# Example tender content (bullet points or lines from the document)\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate bidder's response for SRS document\n",
    "srs_output = generate_bidder_srs_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"SRS-Style Bidder Response:\\n\")\n",
    "print(srs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4c46c",
   "metadata": {},
   "source": [
    "# Version 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c80e5bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS-Style Bidder Response:\n",
      "\n",
      "Project Team members, their roles & responsibilities 3. Approach & methodology to be adopted to implement the Project 4. Risk matrix for all stakeholders 5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # or 't5-large' for potentially better performance\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate bidder's SRS response from tender text\n",
    "def generate_bidder_srs_response(tender_text):\n",
    "    # Revised prompt for clearer instructions\n",
    "    prompt = (\"As a successful bidder, please respond to the following tender requirements in a formal SRS format: \")\n",
    "    \n",
    "    # Combine the prompt with the tender text\n",
    "    input_text = prompt + tender_text\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the bidder's response using T5\n",
    "    outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the output back into text\n",
    "    srs_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return srs_response\n",
    "\n",
    "# Example tender content (bullet points or lines from the document)\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate bidder's response for SRS document\n",
    "srs_output = generate_bidder_srs_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"SRS-Style Bidder Response:\\n\")\n",
    "print(srs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183f675",
   "metadata": {},
   "source": [
    "# Version 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd74553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRS-Style Bidder Response:\n",
      "\n",
      "Project Team members, their roles & responsibilities 2. Approach & methodology to be adopted to implement the Project 3. Responsibility matrix for all stakeholders 4. Risks the bidder anticipates and the plans they have towards their mitigation. 6. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # You can also try 't5-large' for potentially better performance\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate bidder's SRS response from tender text\n",
    "def generate_bidder_srs_response(tender_text):\n",
    "    # Clear and direct prompt\n",
    "    prompt = (\"As a successful bidder, transform the following tender requirements into a formal SRS response:\\n\")\n",
    "    \n",
    "    # Combine the prompt with the tender text\n",
    "    input_text = prompt + tender_text\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the bidder's response using T5\n",
    "    outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the output back into text\n",
    "    srs_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return srs_response\n",
    "\n",
    "# Example tender content (bullet points or lines from the document)\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities\n",
    "2. Approach & methodology to be adopted to implement the Project\n",
    "3. Responsibility matrix for all stakeholders\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate bidder's response for SRS document\n",
    "srs_output = generate_bidder_srs_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"SRS-Style Bidder Response:\\n\")\n",
    "print(srs_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b59c4a",
   "metadata": {},
   "source": [
    "# Version 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef8c3b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rephrased Response:\n",
      "\n",
      "a bidder's commitment: The team will complete the project on time.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # Use 't5-large' for potentially better performance\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a response from input text\n",
    "def generate_response(input_text):\n",
    "    # Clear and specific prompt\n",
    "    prompt = \"Rephrase the following statement as a bidder's commitment: \"\n",
    "    \n",
    "    # Combine the prompt with the input text\n",
    "    input_text = prompt + input_text\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the output back into text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example input text\n",
    "simple_input = \"The team will complete the project on time.\"\n",
    "\n",
    "# Generate response for the simple input\n",
    "simple_output = generate_response(simple_input)\n",
    "\n",
    "# Print the output\n",
    "print(\"Rephrased Response:\\n\")\n",
    "print(simple_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e13a2",
   "metadata": {},
   "source": [
    "# Version 2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2bf50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rephrased Response:\n",
      "\n",
      "A bidder's commitment: The team will complete the project on time.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"  # You can also try 't5-large'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a bidder's commitment response\n",
    "def generate_commitment_response(input_text):\n",
    "    # Clear and concise prompt\n",
    "    prompt = \"Rephrase this as a bidder's commitment: \"\n",
    "    \n",
    "    # Combine the prompt with the input text\n",
    "    input_text = prompt + input_text\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(input_ids, max_length=512, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the output back into text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example input text\n",
    "simple_input = \"The team will complete the project on time.\"\n",
    "\n",
    "# Generate response for the simple input\n",
    "simple_output = generate_commitment_response(simple_input)\n",
    "\n",
    "# Print the output\n",
    "print(\"Rephrased Response:\\n\")\n",
    "print(simple_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f47556",
   "metadata": {},
   "source": [
    "# Version 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66aff313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\adity\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: torch in c:\\users\\adity\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c77d060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c593f4d2fa3647e5a8d652b9c67b854d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49342500afb44724b6adae5d542d5ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5663a3e32330493a899cb9ee5189e064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efce26c1acc4281a2a9f091b298ba89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05e3ad677c845bdb9f40967db6a353c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2512e5059daa4c918e017d9198be90b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrased Response:\n",
      "\n",
      "The team will complete the project on time.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Initialize the Pegasus model and tokenizer\n",
    "model_name = \"google/pegasus-large\"  # You can use other Pegasus variants if needed\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a paraphrased response\n",
    "def generate_paraphrased_response(input_text):\n",
    "    # Prepare the input for the model\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate paraphrase using Pegasus\n",
    "    outputs = model.generate(input_ids, max_length=60, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return paraphrased_text\n",
    "\n",
    "# Example input text\n",
    "simple_input = \"The team will complete the project on time.\"\n",
    "\n",
    "# Generate the paraphrased response\n",
    "paraphrased_output = generate_paraphrased_response(simple_input)\n",
    "\n",
    "# Print the output\n",
    "print(\"Paraphrased Response:\\n\")\n",
    "print(paraphrased_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d461251",
   "metadata": {},
   "source": [
    "# Version 2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "762384c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd37fce2e52b495e9c018e661d84e21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a841d3268d41e296568c32cff86d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b76be6d3b1d4f6ba9b69ad63de65f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8a6a935fea4ed7a3aef360627090aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eae4588b882406a8bb0e32180a7afc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2c3f9e86ab4e698c0571ecc0f4e7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5d7d08e8f44868a74c6bab8b2687c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidder's Response:\n",
      "\n",
      "1. Name of the Project Team members, their roles & responsibilities.\n",
      "\n",
      "2. Approach & methodology to be adopted to implement the Project.\n",
      "\n",
      "3. Responsibility matrix for all stakeholders.\n",
      "\n",
      "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
      "\n",
      "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2-medium\"  # You can use other sizes like 'gpt2', 'gpt2-large', etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a bidder's response\n",
    "def generate_bidder_response(tender_text):\n",
    "    # Clear and direct prompt\n",
    "    prompt = f\"As a successful bidder, provide a detailed response to the following tender requirements:\\n{tender_text}\\nResponse:\"\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(input_ids, max_length=200, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response.split(\"Response:\")[1].strip()\n",
    "\n",
    "# Example tender content\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities.\n",
    "2. Approach & methodology to be adopted to implement the Project.\n",
    "3. Responsibility matrix for all stakeholders.\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the bidder's response for the tender\n",
    "bidder_response = generate_bidder_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"Bidder's Response:\\n\")\n",
    "print(bidder_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1351dbf",
   "metadata": {},
   "source": [
    "# Version 2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aaec4684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89b060ea7144a9bbcca18d186dfb2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c80547671404a78b39dcff07819ae19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce8aaf1650f4e55baf8b640d1ef6242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f55edeea9a4be29fe9244d28bfac58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd82d1cafa204d9e8dca2621c5ebcb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2108ae680b44368fa0cefa1f1797f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794a7026f0e34602a87584aba76b1546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidder's Response:\n",
      "\n",
      "1. Name of the Project Team members, their roles & responsibilities.\n",
      "\n",
      "2. Approach & methodology to be adopted to implement the Project.\n",
      "\n",
      "3. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
      "\n",
      "4. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
      "\n",
      "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2-large\"  # You can use other sizes like 'gpt2', 'gpt2-medium', etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a bidder's response\n",
    "def generate_bidder_response(tender_text):\n",
    "    # Clear and direct prompt\n",
    "    prompt = f\"As a successful bidder, provide a detailed response to the following tender requirements:\\n{tender_text}\\nResponse:\"\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(input_ids, max_length=200, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response.split(\"Response:\")[1].strip()\n",
    "\n",
    "# Example tender content\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities.\n",
    "2. Approach & methodology to be adopted to implement the Project.\n",
    "3. Responsibility matrix for all stakeholders.\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the bidder's response for the tender\n",
    "bidder_response = generate_bidder_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"Bidder's Response:\\n\")\n",
    "print(bidder_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce9de1",
   "metadata": {},
   "source": [
    "# Version 2.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3dac2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8641cf6a50284b89b49921aa5e6026e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4623cb517bd4389bb811ff31c8836f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5d33a149034933b67d8b87b88416b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36dabac755aa4d799fe42121e60cc43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d5225e4b8841f79d6839295a2ab58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt_neo to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10990789fdfe4af7b0b5e99903c748f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "`embed_dim` must be divisible by num_heads (got `embed_dim`: 2048 and `num_heads`: 12).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/gpt-neo-1.3B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Function to generate a bidder's response\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_bidder_response\u001b[39m(tender_text):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Construct a more explicit and contextual prompt\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3886\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3880\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   3881\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   3882\u001b[0m )\n\u001b[0;32m   3884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   3885\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 3886\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3888\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   3889\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1191\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m   1190\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m-> 1191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m GPT2Model(config)\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mn_embd, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:902\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 902\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:902\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 902\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:593\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[1;34m(self, config, layer_idx)\u001b[0m\n\u001b[0;32m    590\u001b[0m attention_class \u001b[38;5;241m=\u001b[39m GPT2_ATTENTION_CLASSES[config\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m--> 593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m attention_class(config\u001b[38;5;241m=\u001b[39mconfig, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39madd_cross_attention:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:466\u001b[0m, in \u001b[0;36mGPT2SdpaAttention.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Idea adapted from transformers.models.bert.modeling_bert.BertSdpaSelfAttention.__init__\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# attn_mask, so we need to call `.contiguous()`. This was fixed in torch==2.2.0.\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_contiguous_qkv \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(get_torch_version()) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:142\u001b[0m, in \u001b[0;36mGPT2Attention.__init__\u001b[1;34m(self, config, is_cross_attention, layer_idx)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`embed_dim` must be divisible by num_heads (got `embed_dim`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and `num_heads`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mscale_attn_weights\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cross_attention \u001b[38;5;241m=\u001b[39m is_cross_attention\n",
      "\u001b[1;31mValueError\u001b[0m: `embed_dim` must be divisible by num_heads (got `embed_dim`: 2048 and `num_heads`: 12)."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a bidder's response\n",
    "def generate_bidder_response(tender_text):\n",
    "    # Construct a more explicit and contextual prompt\n",
    "    prompt = (\n",
    "        \"As a successful bidder responding to the following tender requirements, \"\n",
    "        \"please provide a detailed and professional response:\\n\\n\"\n",
    "        f\"{tender_text}\\n\\n\"\n",
    "        \"Bidder's Response:\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(input_ids, max_length=250, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the actual response from the generated text\n",
    "    return response.split(\"Bidder's Response:\")[1].strip()\n",
    "\n",
    "# Example tender content\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities.\n",
    "2. Approach & methodology to be adopted to implement the Project.\n",
    "3. Responsibility matrix for all stakeholders.\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the bidder's response for the tender\n",
    "bidder_response = generate_bidder_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"Bidder's Response:\\n\")\n",
    "print(bidder_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157cb1b",
   "metadata": {},
   "source": [
    "# Version 2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb018499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidder's Response:\n",
      "\n",
      "1. Names of the Project Team members, their roles & responsibilities.\n",
      "2. Approach & methodology to be adopted to implement the Project.\n",
      "3. Responsibility matrix for all stakeholders.\n",
      "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
      "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a bidder's response\n",
    "def generate_bidder_response(tender_text):\n",
    "    # Construct a more explicit and contextual prompt\n",
    "    prompt = (\n",
    "        \"As a successful bidder responding to the following tender requirements, \"\n",
    "        \"please provide a detailed and professional response:\\n\\n\"\n",
    "        f\"{tender_text}\\n\\n\"\n",
    "        \"Bidder's Response:\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(input_ids, max_length=250, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the actual response from the generated text\n",
    "    return response.split(\"Bidder's Response:\")[1].strip()\n",
    "\n",
    "# Example tender content\n",
    "tender_text = \"\"\"\n",
    "1. Names of the Project Team members, their roles & responsibilities.\n",
    "2. Approach & methodology to be adopted to implement the Project.\n",
    "3. Responsibility matrix for all stakeholders.\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the bidder's response for the tender\n",
    "bidder_response = generate_bidder_response(tender_text)\n",
    "\n",
    "# Print the output\n",
    "print(\"Bidder's Response:\\n\")\n",
    "print(bidder_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030af2a",
   "metadata": {},
   "source": [
    "# Version 2.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1662caf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '\\nAs a successful bidder responding to the following tender requirements, provide a detailed and professional response:\\n1. Names of the Project Team members, their roles & responsibilities.\\n2. Approach & methodology to be adopted to implement the Project.\\n3. Responsibility matrix for all stakeholders.\\n4. Risks the bidder anticipates and the plans they have towards their mitigation.\\n5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\\n6. Status of COADY Members; Official Approvals if any desired.\\n____________________________________________________________________________\\n\\nBy submitting this Competitive Bid Update you intentionally accept and agree to the terms of the Competition Terms & Conditions. The successful bidder, even though the winning bid may not be the first to submit the updated submission, will be the official AIMS:COM Bid Proposal Coordinating Solon Group company, and will have the right to put their own AIMS:COM Marketing Communication Survey results'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-1.3B\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "tender_text = \"\"\"\n",
    "As a successful bidder responding to the following tender requirements, provide a detailed and professional response:\n",
    "1. Names of the Project Team members, their roles & responsibilities.\n",
    "2. Approach & methodology to be adopted to implement the Project.\n",
    "3. Responsibility matrix for all stakeholders.\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "data = query({\"inputs\": tender_text})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ff787",
   "metadata": {},
   "source": [
    "# Version 2.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "296bc080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidder's Response:\n",
      " \n",
      "Tender Requirements:\n",
      "1. Names of the Project Team members, their roles & responsibilities.\n",
      "2. Approach & methodology to be adopted to implement the Project.\n",
      "3. Responsibility matrix for all stakeholders.\n",
      "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
      "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
      "6. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
      "7. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
      "8. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
      "9. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
      "10. Detailed Project Plan, specifying\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-1.3B\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "# Tender requirements example\n",
    "tender_text = \"\"\"\n",
    "Tender Requirements:\n",
    "1. Names of the Project Team members, their roles & responsibilities.\n",
    "2. Approach & methodology to be adopted to implement the Project.\n",
    "3. Responsibility matrix for all stakeholders.\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(\"Error with API request:\", e)\n",
    "        print(\"Response:\", response.json())\n",
    "        return None\n",
    "\n",
    "# Structured prompt for better guidance\n",
    "data = query({\n",
    "    \"inputs\": tender_text,\n",
    "    \"parameters\": {\n",
    "        \"max_length\": 300,\n",
    "        \"temperature\": 0.5,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "})\n",
    "\n",
    "# Check if data was returned successfully\n",
    "if data and isinstance(data, list) and 'generated_text' in data[0]:\n",
    "    print(\"Bidder's Response:\\n\", data[0]['generated_text'])\n",
    "else:\n",
    "    print(\"No valid response received from the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b39e",
   "metadata": {},
   "source": [
    "# Version 2.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cb01c77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Obtaining dependency information for cohere from https://files.pythonhosted.org/packages/b9/a4/48fe3fc5a07a17b0534d5ed4c1f76364e7a9d49487e72636e6f34f2ea8f7/cohere-5.11.1-py3-none-any.whl.metadata\n",
      "  Downloading cohere-5.11.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Obtaining dependency information for fastavro<2.0.0,>=1.9.4 from https://files.pythonhosted.org/packages/43/b3/cac5151810a8c8b5ef318b488a61288fe07e623e9b342c3fc2f60cbfdede/fastavro-1.9.7-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading fastavro-1.9.7-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting httpx>=0.21.2 (from cohere)\n",
      "  Obtaining dependency information for httpx>=0.21.2 from https://files.pythonhosted.org/packages/56/95/9377bcb415797e44274b51d46e3249eba641711cf3348050f76ee7b15ffc/httpx-0.27.2-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere)\n",
      "  Obtaining dependency information for httpx-sse==0.4.0 from https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
      "  Obtaining dependency information for parameterized<0.10.0,>=0.9.0 from https://files.pythonhosted.org/packages/00/2f/804f58f0b856ab3bf21617cccf5b39206e6c4c94c2cd227bde125ea6105f/parameterized-0.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from cohere) (2.6.3)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere)\n",
      "  Obtaining dependency information for pydantic-core<3.0.0,>=2.18.2 from https://files.pythonhosted.org/packages/44/ea/2c5d3da617fdbf3cc0a668f021000a9692a27ed2063dd2889cacdc8af627/pydantic_core-2.25.0-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.25.0-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from cohere) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from cohere) (0.20.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Obtaining dependency information for types-requests<3.0.0,>=2.0.0 from https://files.pythonhosted.org/packages/d7/01/485b3026ff90e5190b5e24f1711522e06c79f4a56c8f4b95848ac072e20f/types_requests-2.32.0.20241016-py3-none-any.whl.metadata\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from cohere) (4.10.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\adity\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\adity\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/06/89/b161908e2f51be56568184aeb4a880fd287178d176fd1c860d2217f41106/httpcore-1.0.6-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\adity\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\adity\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
      "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic>=1.9.2 (from cohere)\n",
      "  Obtaining dependency information for pydantic>=1.9.2 from https://files.pythonhosted.org/packages/df/e4/ba44652d562cbf0bf320e0f3810206149c8a4e99cdbf66da82e97ab53a15/pydantic-2.9.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "     ---------------------------------------- 0.0/149.4 kB ? eta -:--:--\n",
      "     ------------------------------------ - 143.4/149.4 kB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 149.4/149.4 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere)\n",
      "  Obtaining dependency information for pydantic-core<3.0.0,>=2.18.2 from https://files.pythonhosted.org/packages/2f/76/37b7e76c645843ff46c1d73e046207311ef298d3f7b2f7d8f6ac60113071/pydantic_core-2.23.4-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.23.4-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (1.26.18)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tokenizers<1,>=0.15->cohere) (0.23.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.0.0->cohere)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (0.4.6)\n",
      "Downloading cohere-5.11.1-py3-none-any.whl (249 kB)\n",
      "   ---------------------------------------- 0.0/249.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 225.3/249.7 kB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 249.7/249.7 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.9.7-cp311-cp311-win_amd64.whl (500 kB)\n",
      "   ---------------------------------------- 0.0/500.1 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 225.3/500.1 kB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 500.1/500.1 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.0/78.0 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.9 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 286.7/434.9 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 434.9/434.9 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.23.4-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/1.9 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 126.3/126.3 kB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, pydantic-core, parameterized, httpx-sse, httpcore, fastavro, types-requests, pydantic, httpx, cohere\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.16.3\n",
      "    Uninstalling pydantic_core-2.16.3:\n",
      "      Successfully uninstalled pydantic_core-2.16.3\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.6.3\n",
      "    Uninstalling pydantic-2.6.3:\n",
      "      Successfully uninstalled pydantic-2.6.3\n",
      "Successfully installed cohere-5.11.1 fastavro-1.9.7 httpcore-1.0.6 httpx-0.27.2 httpx-sse-0.4.0 parameterized-0.9.0 pydantic-2.9.2 pydantic-core-2.23.4 types-requests-2.32.0.20241016 urllib3-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n",
      "botocore 1.29.76 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.3 which is incompatible.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.3.2 which is incompatible.\n",
      "pandas-profiling 3.2.0 requires visions[type_image_path]==0.7.4, but you have visions 0.7.5 which is incompatible.\n",
      "ydata-profiling 4.6.5 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8894b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidder's Response:\n",
      " # Formal Response to Tender Requirements:\n",
      "\n",
      "## 1. Project Team Structure and Roles:\n",
      "- **Project Manager:** John Smith - Responsible for overall project management, coordination, and client communication. Ensures project goals are met within the defined scope, time, and budget.\n",
      "- **Technical Lead:** Alice Johnson - Leads the technical team, overseeing software architecture, development, and quality assurance. Responsible for designing the technical solution and guiding the development team.\n",
      "- **Senior Developer:** Robert Brown - Expert in software development, focusing on back-end systems. Tasked with developing core functionalities and integrating third-party APIs.\n",
      "- **UI/UX Designer:** Emma Williams - Creates intuitive and visually appealing user interfaces. Responsible for wireframing, prototyping, and designing the user experience.\n",
      "- **Quality Assurance Engineer:** Michael Lee - Ensures the software meets quality standards. Develops test plans, executes tests, and identifies and reports bugs for timely resolution.\n",
      "- **Project Coordinator:**\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "# Enter a valid Cohere API key\n",
    "api_key = 'seaazOBEhAh7z7TTVrHQY3DDWlNrurkwXqMpo7C5'\n",
    "co = cohere.Client(api_key)\n",
    "\n",
    "tender_text = \"\"\"\n",
    "As a successful bidder responding to the following tender requirements, please provide a formal response:\n",
    "1. Names of the Project Team members, their roles & responsibilities.\n",
    "2. Approach & methodology to be adopted to implement the Project.\n",
    "3. Responsibility matrix for all stakeholders.\n",
    "4. Risks the bidder anticipates and the plans they have towards their mitigation.\n",
    "5. Detailed Project Plan, specifying dependencies between various project activities/sub-activities and their timelines.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    response = co.generate(\n",
    "        model='command-xlarge-nightly',  # Use the correct model name\n",
    "        prompt=tender_text,\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    # Print the generated bidder's response\n",
    "    print(\"Bidder's Response:\\n\", response.generations[0].text.strip())\n",
    "\n",
    "# Correct error handling for invalid API tokens and other issues\n",
    "except cohere.CohereError as e:\n",
    "    print(f\"Error with Cohere API request: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5663a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
